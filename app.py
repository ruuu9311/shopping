from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import quote, unquote
import random
import re

# ==========================================
# ğŸ”¥ Selenium ç›¸é—œå¥—ä»¶
# ==========================================
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

app = Flask(__name__)

# ğŸ”¥ è¨­å®šï¼šé€™è£¡æ§åˆ¶æ¯å€‹å¹³å°æœ€å¤šæŠ“å¹¾ç­† (è¨­ç‚º 30)
SEARCH_LIMIT = 30 

# ==========================================

def get_headers(referer="https://www.google.com/"):
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': referer,
    }

def clean_price(price_str):
    if not price_str: return 0
    try:
        if '-' in str(price_str):
            price_str = price_str.split('-')[0]
        clean = ''.join(filter(str.isdigit, str(price_str)))
        return int(clean)
    except:
        return 0

# --- ğŸ”´ PChome çˆ¬èŸ² (è‡ªå‹•ç¿»é ç‰ˆ) ---
def scrape_pchome(keyword):
    print(">>> æ­£åœ¨çˆ¬å– PChome...")
    results = []
    page = 1 # å¾ç¬¬ä¸€é é–‹å§‹
    
    try:
        # ä½¿ç”¨ while è¿´åœˆï¼šåªè¦æŠ“åˆ°çš„æ•¸é‡é‚„æ²’æ»¿ 30 ç­†ï¼Œå°±ç¹¼çºŒæŠ“ä¸‹ä¸€é 
        while len(results) < SEARCH_LIMIT:
            url = f"https://ecshweb.pchome.com.tw/search/v3.3/all/results?q={quote(keyword)}&page={page}&sort=sale/dc"
            r = requests.get(url, headers=get_headers(), timeout=10)
            
            if r.status_code != 200: break

            data = r.json()
            if 'prods' not in data or not data['prods']:
                break # å¦‚æœæ²’è³‡æ–™äº†å°±åœæ­¢

            for item in data['prods']:
                # ğŸ”¥ å†æ¬¡æª¢æŸ¥ï¼šå¦‚æœå·²ç¶“æ»¿ 30 ç­†ï¼Œå°±é¦¬ä¸Šåœæ­¢
                if len(results) >= SEARCH_LIMIT: break
                
                pic = item.get('picS')
                results.append({
                    'platform': 'PChome',
                    'name': item.get('name'),
                    'price': float(item.get('price', 0)),
                    'link': f"https://24h.pchome.com.tw/prod/{item.get('Id')}",
                    'img': f"https://cs-a.ecimg.tw{pic}" if pic else "",
                    'sales': '24håˆ°è²¨',
                    'rating': 4.5
                })
            
            page += 1 # æº–å‚™æŠ“ä¸‹ä¸€é 
            
        print(f"âœ… PChome æˆåŠŸ: æŠ“åˆ° {len(results)} ç­†")
        return results
    except Exception as e:
        print(f"âŒ PChome éŒ¯èª¤: {e}")
        return []

# --- ğŸ©· MOMO çˆ¬èŸ² (Selenium å…¨èƒ½ç‰ˆ) ---
def scrape_momo(keyword):
    print(">>> æ­£åœ¨çˆ¬å– MOMO (Selenium å…¨èƒ½ç‰ˆ)...")
    driver = None
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless") 
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")

        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"})

        url = f"https://www.momoshop.com.tw/search/{quote(keyword)}?viewport=desktop&_isFuzzy=0&searchType=1&cateLevel=0"
        driver.get(url)

        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(2)
        except: pass

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = []

        products = soup.select('li.listAreaLi')
        if not products: products = soup.select('.goodsItemLi')
        if not products: products = soup.select('.goods-mobile-panel__item-content')

        if not products:
            print("ğŸ”„ MOMO åˆæ¬¡æŠ“å–å¤±æ•—ï¼Œå˜—è©¦é‡æ–°æ•´ç†...")
            driver.refresh()
            time.sleep(4)
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            products = soup.select('li.listAreaLi') or soup.select('.goodsItemLi') or soup.select('.goods-mobile-panel__item-content')

        for p in products:
            # ğŸ”¥ MOMO ç…è»Šæ©Ÿåˆ¶
            if len(results) >= SEARCH_LIMIT: break

            try:
                name, price, link, img_src = "æœªçŸ¥", 0, "#", ""
                
                if 'listAreaLi' in str(p.get('class', [])):
                    name_tag = p.select_one('.prdName') or p.select_one('h3')
                    price_tag = p.select_one('.price') or p.select_one('.money')
                    link_tag = p.select_one('a.goods-img-url') or p.select_one('a')
                    img_tag = p.select_one('img.goods-img') or p.select_one('img')
                elif 'goods-mobile-panel' in str(p.get('class', [])):
                    name_tag = p.select_one('.content-info__goods-name') or p.select_one('h3')
                    price_tag = p.select_one('.price-group__current-value') or p.select_one('.price')
                    link_tag = p.select_one('a')
                    img_tag = p.select_one('img')
                else:
                    name_tag = p.select_one('.prdName') or p.select_one('h3')
                    price_tag = p.select_one('.price') or p.select_one('.money')
                    link_tag = p.select_one('a')
                    img_tag = p.select_one('img')

                if name_tag and price_tag:
                    name = name_tag.text.strip()
                    price = clean_price(price_tag.text)
                    if price == 0: continue

                    if link_tag and link_tag.has_attr('href'):
                        link = link_tag['href']
                        if not link.startswith('http'): link = "https://www.momoshop.com.tw" + link
                    
                    if img_tag:
                        img_src = img_tag.get('src')
                        if not img_src or 'blank' in img_src or 'loading' in img_src: 
                            img_src = img_tag.get('data-original') or img_tag.get('data-src')

                    results.append({'platform': 'MOMO', 'name': name, 'price': price, 'link': link, 'img': img_src if img_src else "", 'sales': 'ç†±éŠ·', 'rating': 4.0})
            except: continue
            
        print(f"âœ… MOMO æˆåŠŸ: æŠ“åˆ° {len(results)} ç­†")
        return results
    except Exception as e:
        print(f"âŒ MOMO éŒ¯èª¤: {e}")
        return []
    finally:
        if driver: driver.quit()

# --- ğŸ“š åšå®¢ä¾† çˆ¬èŸ² (Regex åŸå§‹åœ–é‚„åŸç‰ˆ) ---
def scrape_books(keyword):
    print(">>> æ­£åœ¨çˆ¬å– åšå®¢ä¾† (åœ–ç‰‡å¼·åˆ¶é‚„åŸç‰ˆ)...")
    driver = None
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless") 
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")

        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {"source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"})

        url = f"https://search.books.com.tw/search/query/key/{quote(keyword)}/cat/all"
        driver.get(url)

        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, ".table-td, .item")))
            driver.execute_script("window.scrollBy(0, 500);")
            time.sleep(2)
        except: pass

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = []
        
        candidates = soup.select('.table-td')
        if not candidates: candidates = soup.select('.item')
        if not candidates:
             box_divs = soup.select('div.box')
             candidates = [div.parent for div in box_divs]

        for tag in candidates:
            # ğŸ”¥ åšå®¢ä¾† ç…è»Šæ©Ÿåˆ¶
            if len(results) >= SEARCH_LIMIT: break

            try:
                name_tag = tag.select_one('h4 a') or tag.select_one('h3 a') or tag.select_one('a[title]')
                
                # åƒ¹æ ¼è™•ç†
                price = 0
                price_row = tag.select_one('ul.price') or tag.select_one('.price')
                if price_row:
                    price_text = price_row.get_text()
                    matches = re.findall(r'(\d+(?:,\d+)*)\s*å…ƒ', price_text)
                    if matches: price = clean_price(matches[-1])
                    else:
                        all_nums = re.findall(r'\d+', price_text)
                        valid = [int(n) for n in all_nums if int(n) > 10]
                        if valid: price = valid[-1]
                if price == 0:
                    pt = tag.select_one('ul.price li b') or tag.select_one('.price strong')
                    if pt: price = clean_price(pt.text)

                img_tag = tag.select_one('div.box img') or tag.select_one('img')

                if name_tag and price > 0:
                    name = name_tag.get('title') or name_tag.text.strip()
                    link = name_tag.get('href')
                    if link:
                        if link.startswith('//'): link = "https:" + link
                        elif not link.startswith('http'): link = "https://search.books.com.tw" + link

                    img_src = ""
                    if img_tag:
                        raw_html = str(img_tag)
                        decoded_html = unquote(raw_html)
                        match = re.search(r'(https?://(?:www\.|im1\.)?books\.com\.tw/img/[^"\']+\.(?:jpg|png|jpeg))', decoded_html)
                        
                        if match:
                            img_src = match.group(1)
                        else:
                            img_src = img_tag.get('data-original') or img_tag.get('src')
                            if img_src:
                                img_src = unquote(img_src)
                                if 'getImage' in img_src:
                                    try:
                                        img_src = img_src.split('i=')[1].split('&')[0]
                                    except: pass

                        if img_src:
                            if img_src.startswith('//'): img_src = "https:" + img_src
                            if 'blank' in img_src or 'loading' in img_src: img_src = ""

                    if not img_src:
                        img_src = "https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/no-image.jpg"

                    results.append({'platform': 'åšå®¢ä¾†', 'name': name, 'price': price, 'link': link, 'img': img_src, 'sales': 'æ¨è–¦', 'rating': 4.2})
            except: continue
            
        print(f"âœ… åšå®¢ä¾† æˆåŠŸ: æŠ“åˆ° {len(results)} ç­†")
        return results
    except Exception as e:
        print(f"âŒ åšå®¢ä¾† éŒ¯èª¤: {e}")
        return []
    finally:
        if driver: driver.quit()

@app.route('/')
def home():
    return render_template('shop.html')

@app.route('/api/search')
def api_search():
    keyword = request.args.get('q', '')
    if not keyword: return jsonify([])

    print(f"\nğŸš€ [é–‹å§‹æœå°‹] é—œéµå­—: {keyword}")
    results = []
    
    results.extend(scrape_pchome(keyword))
    results.extend(scrape_momo(keyword))
    results.extend(scrape_books(keyword))

    results.sort(key=lambda x: x['price'])
    return jsonify(results)

if __name__ == '__main__':
    app.run(debug=True, port=5000)